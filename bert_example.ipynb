{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 输入表示"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"获取输入序列的词元及其片段索引\"\"\"\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 0和1分别标记片段A和B\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_a) + 1)\n",
    "    return tokens, segments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**BERTEncoder类。与TransformerEncoder不同，BERTEncoder使⽤⽚段嵌⼊和可学习的位置嵌⼊。**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"Bert编码器\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n",
    "                 dropout, max_len=1000, key_size=768, query_size=768, value_size=768, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f'{i}', d2l.EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                                                          ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
    "            # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置参数\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens))\n",
    "            # print(f'pos_embedding: {self.pos_embedding}')\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # 在以下代码段中，X的形状保持不变：(批量大小，最大序列长度，num_hiddens)\n",
    "        # print(f'tokens: {tokens}')\n",
    "        # print(f'segments: {segments}')\n",
    "        # print(f'valid_lens: {valid_lens}')\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        # print(f'tokens and segments embedding: {X}')\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        # print(f'tokens and segments and pos embedding: {X}')\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "            # print(f'X in blk: {X}')\n",
    "        return X\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**假设词表⼤⼩为10000，为了演⽰BERTEncoder的前向推断，让我们创建⼀个实例并初始化它的参数**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
    "norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                      ffn_num_hiddens, num_heads, num_layers, dropout)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 8, 768])"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = torch.randint(0, vocab_size, (2, 8))\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "encoded_X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_X: tensor([[[-1.3943,  0.0458, -0.5456,  ...,  0.2800,  0.9279,  0.0133],\n",
      "         [ 0.4907, -1.6218, -0.5231,  ..., -0.8197,  1.4296,  0.7573],\n",
      "         [ 0.0541, -2.1216,  0.6969,  ..., -0.2749,  0.4089,  0.7051],\n",
      "         ...,\n",
      "         [-1.9607, -2.1127, -0.8636,  ..., -1.3949,  1.0475, -1.3129],\n",
      "         [-1.2140, -0.5986,  0.2625,  ..., -2.6860, -0.6815, -0.4885],\n",
      "         [-0.1200, -1.2591,  0.6356,  ..., -0.0253, -0.3451, -0.3682]],\n",
      "\n",
      "        [[-1.6944,  0.4904,  1.3444,  ..., -0.9685,  0.9147,  0.1177],\n",
      "         [-0.5462, -1.6796, -0.4770,  ..., -0.8922, -0.5093, -0.3878],\n",
      "         [-0.1904,  0.0251, -1.6325,  ..., -0.3845, -0.5029,  0.5741],\n",
      "         ...,\n",
      "         [ 0.1681, -0.7113, -1.0024,  ..., -1.1642,  0.8973, -1.2125],\n",
      "         [-0.4128,  0.0712,  1.0136,  ..., -1.4053, -0.3935, -0.8266],\n",
      "         [-1.3334, -1.0205,  0.0063,  ..., -1.3598, -1.1006,  1.1586]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'encoded_X: {encoded_X}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 预训练任务\n",
    "**BERTEncoder的前向推断给出了输⼊⽂本的每个词元和插⼊的特殊标记“<cls>”及“<seq>”的BERT表⽰。\n",
    "接下来，我们将使⽤这些表⽰来计算预训练BERT的损失函数。预训练包括以下两个任务：掩蔽语⾔模型和下\n",
    "⼀句预测。**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 遮蔽语言模型（Masked Language Modeling）\n",
    "**语⾔模型使⽤左侧的上下⽂预测词元。为了双向编码上下⽂以表⽰每个词元，BERT随机掩蔽词元并使⽤来⾃双向上下⽂的词元以⾃监督的⽅式预测掩蔽词元。此任务称为掩蔽语⾔模型。**\n",
    "**在这个预训练任务中，将随机选择15%的词元作为预测的掩蔽词元。要预测⼀个掩蔽词元⽽不使⽤标签作弊，⼀个简单的⽅法是总是⽤⼀个特殊的“<mask>”替换输⼊序列中的词元。然⽽，⼈造特殊词元“<mask>”不会出现在微调中。为了避免预训练和微调之间的这种不匹配，如果为预测⽽屏蔽词元（例如，在“this movie is great”中选择掩蔽和预测“great”），则在输⼊中将其替换为：\n",
    "• 80%时间为特殊的“<mask>“词元（例如，“this movie is great”变为“this movie is<mask>”；\n",
    "• 10%时间为随机词元（例如，“this movie is great”变为“this movie is drink”）；\n",
    "• 10%时间内为不变的标签词元（例如，“this movie is great”变为“this movie is great”）。\n",
    "请注意，在15%的时间中，有10%的时间插⼊了随机词元。这种偶然的噪声⿎励BERT在其双向上下⽂编码中不那么偏向于掩蔽词元（尤其是当标签词元保持不变时）。**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**我们实现了下⾯的MaskLM类来预测BERT预训练的掩蔽语⾔模型任务中的掩蔽标记。预测使⽤单隐藏层的多层感知机（self.mlp）。在前向推断中，它需要两个输⼊：BERTEncoder的编码结果和⽤于预测的词元位置。输出是这些位置的预测结果。**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    \"\"\"BERT的遮蔽语言模型任务\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                 nn.Linear(num_hiddens, vocab_size))\n",
    "\n",
    "    def forward(self, X, pred_positions):\n",
    "        num_pred_position = pred_positions.shape[1]\n",
    "        print(f'num_pred_position: {num_pred_position}')\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        print(f'pred_positions: {pred_positions}')\n",
    "        batch_size = X.shape[0]\n",
    "        print(f'batch_size: {batch_size}')\n",
    "        batch_idx = torch.arange(0, batch_size)\n",
    "        # 假设batch_size=2，num_pred_positions=3\n",
    "        # 那么batch_idx是np.array（[0,0,0,1,1,1]）\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_position)\n",
    "        print(f'batch_idx: {batch_idx}')\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        print(f'masked_X: {masked_X}')\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_position, -1))\n",
    "        print(f'masked_X: {masked_X}')\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**为了演⽰MaskLM的前向推断，我们创建了其实例mlm并对其进⾏了初始化。回想⼀下，来⾃BERTEncoder的正向推断encoded_X表⽰2个BERT输⼊序列。我们将mlm_positions定义为在encoded_X的任⼀输⼊序列中预测的3个指⽰。mlm的前向推断返回encoded_X的所有掩蔽位置mlm_positions处的预测结果mlm_Y_hat。对于每个预测，结果的⼤⼩等于词表的⼤⼩。**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_pred_position: 3\n",
      "pred_positions: tensor([1, 5, 2, 6, 1, 5])\n",
      "batch_size: 2\n",
      "batch_idx: tensor([0, 0, 0, 1, 1, 1])\n",
      "masked_X: tensor([[ 0.4907, -1.6218, -0.5231,  ..., -0.8197,  1.4296,  0.7573],\n",
      "        [-1.9607, -2.1127, -0.8636,  ..., -1.3949,  1.0475, -1.3129],\n",
      "        [ 0.0541, -2.1216,  0.6969,  ..., -0.2749,  0.4089,  0.7051],\n",
      "        [-0.4128,  0.0712,  1.0136,  ..., -1.4053, -0.3935, -0.8266],\n",
      "        [-0.5462, -1.6796, -0.4770,  ..., -0.8922, -0.5093, -0.3878],\n",
      "        [ 0.1681, -0.7113, -1.0024,  ..., -1.1642,  0.8973, -1.2125]],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "masked_X: tensor([[[ 0.4907, -1.6218, -0.5231,  ..., -0.8197,  1.4296,  0.7573],\n",
      "         [-1.9607, -2.1127, -0.8636,  ..., -1.3949,  1.0475, -1.3129],\n",
      "         [ 0.0541, -2.1216,  0.6969,  ..., -0.2749,  0.4089,  0.7051]],\n",
      "\n",
      "        [[-0.4128,  0.0712,  1.0136,  ..., -1.4053, -0.3935, -0.8266],\n",
      "         [-0.5462, -1.6796, -0.4770,  ..., -0.8922, -0.5093, -0.3878],\n",
      "         [ 0.1681, -0.7113, -1.0024,  ..., -1.1642,  0.8973, -1.2125]]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([2, 3, 10000])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "mlm_position = torch.tensor([[1, 5, 2], [6, 1, 5]])\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_position)\n",
    "mlm_Y_hat.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm_Y_hat: tensor([[[-0.7321,  0.2989, -0.0655,  ..., -0.0333, -0.2578,  0.9658],\n",
      "         [-0.5161, -0.1170, -0.3803,  ...,  0.4254, -0.1207, -0.7576],\n",
      "         [-0.4004, -0.0165,  0.9563,  ...,  0.0673,  0.4122,  0.7785]],\n",
      "\n",
      "        [[ 0.2433,  1.0621,  1.3504,  ..., -0.6983, -0.3634,  0.0222],\n",
      "         [ 0.7957,  0.2007,  0.2906,  ...,  0.7890,  0.6155,  0.4064],\n",
      "         [ 0.3141,  0.5112,  0.1292,  ..., -0.1900, -0.3586, -0.2589]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'mlm_Y_hat: {mlm_Y_hat}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**通过掩码下的预测词元mlm_Y的真实标签mlm_Y_hat，我们可以计算在BERT预训练中的遮蔽语⾔模型任务\n",
    "的交叉熵损失。**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: tensor([[ 636, 6395, 7491, 9497, 7918, 2362, 6437, 5562],\n",
      "        [7906,  485, 9052, 3778, 9504, 5929, 1732, 8488]])\n"
     ]
    }
   ],
   "source": [
    "print(f'tokens: {tokens}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([6])"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
    "mlm_l.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 下⼀句预测（Next Sentence Prediction）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**尽管掩蔽语⾔建模能够编码双向上下⽂来表⽰单词，但它不能显式地建模⽂本对之间的逻辑关系。为了帮助\n",
    "理解两个⽂本序列之间的关系，BERT在预训练中考虑了⼀个⼆元分类任务——下⼀句预测。在为预训练⽣成\n",
    "句⼦对时，有⼀半的时间它们确实是标签为“真”的连续句⼦；在另⼀半的时间⾥，第⼆个句⼦是从语料库\n",
    "中随机抽取的，标记为“假”。**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**下⾯的NextSentencePred类使⽤单隐藏层的多层感知机来预测第⼆个句⼦是否是BERT输⼊序列中第⼀\n",
    "个句⼦的下⼀个句⼦。由于Transformer编码器中的⾃注意⼒，特殊词元“<cls>”的BERT表⽰已经对输⼊的\n",
    "两个句⼦进⾏了编码。因此，多层感知机分类器的输出层（self.output）以X作为输⼊，其中X是多层感\n",
    "知机隐藏层的输出，⽽MLP隐藏层的输⼊是编码后的“<cls>”词元。**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"BERT的下一句预测任务\"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        self.output = nn.Linear(num_inputs, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # X的形状：（batch_size, num_hiddens）\n",
    "        return self.output(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**可以看到，NextSentencePred实例的前向推断返回每个BERT输⼊序列的⼆分类预测。**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 2])"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_nsp = encoded_X\n",
    "encoded_X_nsp = torch.flatten(encoded_X_nsp, start_dim=1)\n",
    "# NSP的输入形状：（batch_size,num_hiddens）\n",
    "nsp = NextSentencePred(encoded_X_nsp.shape[-1])\n",
    "nsp_Y_hat = nsp(encoded_X_nsp)\n",
    "nsp_Y_hat.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "print(encoded_X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 6144])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_nsp.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2])"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsp_y = torch.tensor([0, 1])\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "nsp_l.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0632, 0.8091], grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(nsp_l)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**值得注意的是，上述两个预训练任务中的所有标签都可以从预训练语料库中获得，⽽⽆需⼈⼯标注。原始\n",
    "的BERT已经在图书语料库 [Zhu et al., 2015]和英⽂维基百科的连接上进⾏了预训练。这两个⽂本语料库⾮常\n",
    "庞⼤：它们分别有8亿个单词和25亿个单词。**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 整合代码"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**在预训练BERT时，最终的损失函数是掩蔽语⾔模型损失函数和下⼀句预测损失函数的线性组合。现在我们\n",
    "可以通过实例化三个类BERTEncoder、MaskLM和NextSentencePred来定义BERTModel类。前向推断返\n",
    "回编码后的BERT表⽰encoded_X、掩蔽语⾔模型预测mlm_Y_hat和下⼀句预测nsp_Y_hat。**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    \"\"\"BERT模型\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n",
    "                 dropout, max_len=1000, key_size=768, query_size=768, value_size=768, hid_in_features=768,\n",
    "                 mlm_in_features=768, nsp_in_features=768):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "                                   num_layers, dropout, max_len=max_len, key_size=key_size, query_size=query_size,\n",
    "                                   value_size=value_size)\n",
    "        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens), nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
    "        self.nsp = NextSentencePred(nsp_in_features)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "py37",
   "language": "python",
   "display_name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}